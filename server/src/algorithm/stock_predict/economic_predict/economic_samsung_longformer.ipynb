{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import LongformerModel, LongformerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "stock_df = pd.read_excel('../../../data/tb_stock.xlsx')\n",
    "main_economic_df = pd.read_excel('../../../data/tb_main_economic_index.xlsx')\n",
    "korea_economic_df = pd.read_excel('../../../data/tb_korea_economic_indicator.xlsx')\n",
    "\n",
    "#stock_df  = stock_df[-365:]\n",
    "#main_economic_df  = main_economic_df[-365:]\n",
    "#korea_economic_df  = korea_economic_df[-365:]\n",
    "\n",
    "# 필요한 열만 선택\n",
    "stock_df = stock_df[['sc_date', 'sc_ss_stock']]\n",
    "main_economic_df = main_economic_df[['mei_date', 'mei_gold', 'mei_sp500', 'mei_kospi']]\n",
    "korea_economic_df = korea_economic_df[['kei_date', 'kei_m2_avg', 'kei_fr']]\n",
    "\n",
    "# 열 이름 변경\n",
    "stock_df.rename(columns={'sc_date': 'date'}, inplace=True)\n",
    "main_economic_df.rename(columns={'mei_date': 'date'}, inplace=True)\n",
    "korea_economic_df.rename(columns={'kei_date': 'date'}, inplace=True)\n",
    "\n",
    "# 데이터프레임 병합\n",
    "merged_df = pd.merge(stock_df, main_economic_df, on='date', how='inner')\n",
    "merged_df = pd.merge(merged_df, korea_economic_df, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date                                               text  target\n",
      "0 2014-09-17  On 2014-09-17, gold price was 1234.40002441406...   24520\n",
      "1 2014-09-18  On 2014-09-18, gold price was 1225.69995117187...   24200\n",
      "2 2014-09-19  On 2014-09-19, gold price was 1215.30004882812...   24200\n",
      "3 2014-09-20  On 2014-09-20, gold price was 1215.30004882812...   24200\n",
      "4 2014-09-21  On 2014-09-21, gold price was 1215.30004882812...   24200\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 생성\n",
    "merged_df['text'] = merged_df.apply(lambda row: f\"On {row['date']}, gold price was {row['mei_gold']}, S&P 500 index was {row['mei_sp500']}, KOSPI index was {row['mei_kospi']}, M2 average was {row['kei_m2_avg']}, and FR was {row['kei_fr']}.\", axis=1)\n",
    "\n",
    "# 모델의 타겟 설정\n",
    "merged_df['target'] = merged_df['sc_ss_stock']\n",
    "\n",
    "# 날짜 형식 확인 및 변환\n",
    "if not pd.api.types.is_datetime64_any_dtype(merged_df['date']):\n",
    "         merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "print(merged_df[['date', 'text', 'target']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 정의\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length                              # 토큰화 사용시 최대 시퀀스 길이를 저장\n",
    "\n",
    "    def __len__(self):                                            # 데이터의 개수를 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):                                   # text, target 반환\n",
    "        text   = self.data.iloc[idx]['text']\n",
    "        target = self.data.iloc[idx]['target']\n",
    "        \n",
    "        inputs = self.tokenizer(text, \n",
    "                                truncation=True,                  # 시퀀스가 max_length를 초과할 경우 자릅니다.\n",
    "                                padding   ='max_length',          # 시퀀스가 max_length에 맞게 패딩\n",
    "                                max_length=self.max_length,       # 시퀀스 최대길이 설정\n",
    "                                return_tensors=\"pt\")              # 결과를 pytorch 텐서 형식으로 반환\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),           # 토큰 ID 배열을 반환\n",
    "            'attention_mask': inputs['attention_mask'].flatten(), # 패딩된 토큰을 무시하기 위한 마스크 배열 반환\n",
    "            'target': torch.tensor(target, dtype=torch.float)     # 타겟ㅇ르 float 타입의 텐서로 반환\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pruning을 적용하는 모델 정의\n",
    "class PrunedStockPricePredictor(nn.Module):\n",
    "    # 클래스 초기화 메서드\n",
    "    def __init__(self, longformer_model_name):\n",
    "        # super은 부모 혹은 상위 클래스를 호출하여 기능을 사용하기 위해 쓰일 수 있다.\n",
    "        super(PrunedStockPricePredictor, self).__init__()\n",
    "        # 사전 학습된 longformermodel을 불러올 수 있다.\n",
    "        self.longformer = LongformerModel.from_pretrained(longformer_model_name)\n",
    "        # Fully Connected Layer으로 벡터화, 특징 추출의 마지막 단계이다.\n",
    "        self.fc         = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "\n",
    "    # 모델의 순전파 과정을 정의\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 모델의 출력에서 첫 번째 토큰([CLS] 토큰)의 벡터를 가져온다.\n",
    "        # outputs[0]은 마지막 레이어의 은닉 상태 텐서를 의미하며, 모든 토큰에 대한 정보를 포함\n",
    "        # 크키 : [batch_size, sequence_length, hidden_size]\n",
    "        cls_output = outputs[0][:, 0, :]\n",
    "        return self.fc(cls_output)\n",
    "    \n",
    "    def apply_pruning(self, pruning_amount=0.4):\n",
    "        # Fully connected layer에 L1 가지치기 적용\n",
    "        # 가지치기를 적용할 대상이 가중치(weight)\n",
    "        prune.l1_unstructured(self.fc, name=\"weight\", amount=pruning_amount)\n",
    "        # 가지치기 적용 후 pruned 상태에서 재학습을 위해 제거\n",
    "        prune.remove(self.fc, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. 데이터 준비\n",
    "tokenizer         = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "max_length        = 512\n",
    "\n",
    "train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "train_dataset     = StockDataset(train_df, tokenizer, max_length)\n",
    "test_dataset      = StockDataset(test_df, tokenizer, max_length)\n",
    "\n",
    "# 데이터셋을 배치 단위로 로드\n",
    "# 학습 시에는 무작위로 일반화 능력을 높이고,\n",
    "# 테스트 시에는 섞지 않아 일관된 평가를 보장\n",
    "train_loader      = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader       = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 데이터 분할 및 변수 초기화\n",
    "\n",
    "X_train = np.array([\n",
    "    tokenizer(text, truncation=True, padding='max_length', max_length=max_length, \n",
    "              return_tensors='pt')\n",
    "              ['input_ids'].        # 'input_ids'로 반환(토큰의 ID)\n",
    "              flatten().            # 2차원 텐서로 변환 [batch_size, sequence_length]의 형태\n",
    "              numpy()               # PyTorch 텐서를 NumPy 배열로 변환\n",
    "    for text in train_df['text']\n",
    "])\n",
    "\n",
    "y_train = train_df['target'].values\n",
    "\n",
    "# 훈련 데이터와 동일한 방식으로 토큰화 및 패딩을 수행\n",
    "X_val = np.array([\n",
    "    tokenizer(text, truncation=True, padding='max_length', max_length=max_length, \n",
    "              return_tensors='pt')\n",
    "              ['input_ids'].        # 'input_ids'로 반환(토큰의 ID)\n",
    "              flatten().            # 2차원 텐서로 변환 [batch_size, sequence_length]의 형태\n",
    "              numpy()               # PyTorch 텐서를 NumPy 배열로 변환\n",
    "    for text in train_df['text']\n",
    "])\n",
    "\n",
    "y_val = train_df['target'].values\n",
    "\n",
    "# 모델 저장\n",
    "best_model_path = '../saved_models/EconomicPredict_Smsung_Longformer_best.pt'\n",
    "\n",
    "# 모델 저장 경로의 디렉토리를 생성합니다.\n",
    "# os.makedirs는 디렉토리가 없을 경우 생성하며, exist_ok=True로 이미 존재하는 경우 에러를 방지합니다.\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "\n",
    "# 파라미터 리스트\n",
    "learning_rates = [1e-5, 3e-5, 5e-5]\n",
    "model_names = ['allenai/longformer-base-4096', 'allenai/longformer-large-4096']\n",
    "\n",
    "# 최적의 모델을 찾기 위해 초기 최적 점수를 무한대로 설정\n",
    "# 이 변수는 나중에 검증 손실이 현재 최적 점수보다 낮은 경우 업데이트\n",
    "best_score = float('inf')\n",
    "\n",
    "# 학습과 정에서 발생하는 훈련 손실 값을 저장하는 리스트\n",
    "train_losses = []\n",
    "\n",
    "# 학습 과정에서 발생하는 검증 손실 값을 저장할 리스트\n",
    "val_losses   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "최고의 학습률:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=1e-05, model_name=allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 3095903484.1104975, Validation Loss: 3095573606.8066297\n"
     ]
    }
   ],
   "source": [
    "# 5. 최적의 하이퍼 파라미터 찾기\n",
    "for lr in tqdm(learning_rates, desc='최고의 학습률'):\n",
    "    for model_name in tqdm(model_names, desc='최고의 모델'):\n",
    "        print(f\"Training with lr={lr}, model_name={model_name}\")\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = PrunedStockPricePredictor(model_name)\n",
    "        model.apply_pruning(pruning_amount=0.4)\n",
    "        model.train()\n",
    "\n",
    "        # Adam은 각 파라미터에 대해 개별 학습률을 유지하고 업데이트하며, 학습 중에 학습률을 조정\n",
    "        # 기울기 값을 고려하면서 파라미터를 업데이트\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 손실 함수로 MSE를 사용 (평균 제곱 오차)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        train_loss_per_epoch = 0\n",
    "        val_loss_per_epoch = 0\n",
    "\n",
    "        for epoch in tqdm(range(3)):\n",
    "            model.train()\n",
    "\n",
    "            # 현재 에포크 동안의 누적 훈련 손실을 저장할 변수를 초기화\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # 훈련 데이터셋을 배치 단위로 나누어 학습\n",
    "            for i in range(0, len(X_train), 4):\n",
    "                # 입력 데이터와 타깃 데이터를 텐서로 변환, 모델이 사용 중인 디바이스로 전환\n",
    "                input_ids = torch.tensor(X_train[i:i+4]).to(model.longformer.device)\n",
    "                attention_mask = (input_ids != 0).long().to(model.longformer.device)\n",
    "                targets = torch.tensor(y_train[i:i+4], dtype=torch.float).to(model.longformer.device)\n",
    "                \n",
    "                # 옵티마이저 기울기를 초기화\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 모델을 통해 예측값을 계산\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                # 에측값과 실제값 사이의 손실(loss)을 계산합니다.\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "                # 역전파를 통해 기울기를 계산하고, 옵티마이저가 이를 기반으로 파라미터를 업데이트\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 현재 배치의 손실을 누적\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # 에포크가 끝날 때마다 평균 훈련 손실을 계산하고, 리스트에 추가\n",
    "            train_loss_per_epoch = running_loss / len(X_train) * 4\n",
    "            train_losses.append(train_loss_per_epoch)\n",
    "\n",
    "            # 검증 단계\n",
    "            model.eval()\n",
    "            val_loss = 0.0        # 현재 에포크 동안의 누적 검증 손실을 저장할 변수\n",
    "            with torch.no_grad(): # 평가단계에서는 기울기를 계산하지 않는다.\n",
    "                for i in range(0, len(X_val), 4):\n",
    "                    # 입력, 타겟 데이터를 텐서로 변환하고,  모델이 사용 중인 디바이스로 전송\n",
    "                    input_ids = torch.tensor(X_val[i:i+4]).to(model.longformer.device)\n",
    "                    attention_mask = (input_ids != 0).long().to(model.longformer.device)\n",
    "                    targets = torch.tensor(y_val[i:i+4], dtype=torch.float).to(model.longformer.device)\n",
    "                    \n",
    "                    # 검증 데이터에 대한 예측값을 계산합니다.\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                    # 예측값과 실제값 사이의 손실을 계산\n",
    "                    loss = criterion(outputs.squeeze(), targets)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            val_loss_per_epoch = val_loss / len(X_val) * 4\n",
    "            val_losses.append(val_loss_per_epoch)\n",
    "\n",
    "            print(f'Epoch {epoch+1}: Training Loss: {train_loss_per_epoch}, Validation Loss: {val_loss_per_epoch}')\n",
    "\n",
    "        # 최적의 모델 저장\n",
    "        if val_loss_per_epoch < best_score:\n",
    "            best_score = val_loss_per_epoch\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "print(f\"Best Validation Loss: {best_score}\")\n",
    "print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "# 드룹아웃, 배치 정규화 등 레이어들을 비활성화\n",
    "model.eval()\n",
    "y_train_pred = []\n",
    "y_val_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_train), 4):\n",
    "        input_ids = torch.tensor(X_train[i:i+4]).to(model.longformer.device)\n",
    "        # attention_mask는 입력 데이터에서 패딩 토큰(0)을 무시하는 역할\n",
    "        attention_mask = (input_ids != 0).long().to(model.longformer.device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        y_train_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    for i in range(0, len(X_val), 4):\n",
    "        input_ids = torch.tensor(X_val[i:i+4]).to(model.longformer.device)\n",
    "        attention_mask = (input_ids != 0).long().to(model.longformer.device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        y_val_pred.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 계산\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}, RMSE: {train_rmse}, MAE: {train_mae}\")\n",
    "print(f\"Validation MSE: {val_mse}, RMSE: {val_rmse}, MAE: {val_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 과적합 여부 확인\n",
    "if min(val_mse, train_mse) < min(train_mse, val_mse):\n",
    "    print(\"경고: 과적합이 발생할 가능성이 있습니다. 검증 손실이 훈련 손실보다 낮습니다.\")\n",
    "elif val_rmse > train_rmse:\n",
    "    print(\"경고: 과적합 가능성이 있습니다. 검증 손실이 훈련 손실보다 높습니다.\")\n",
    "else:\n",
    "    print(\"모델이 적절하게 학습되었습니다. 과적합이 크게 발생하지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 검증 손실의 변화 추이 그래프\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잔차 계산\n",
    "train_residuals = y_train - np.array(y_train_pred).flatten()\n",
    "val_residuals = y_val - np.array(y_val_pred).flatten()\n",
    "\n",
    "# 잔차 플롯 (훈련 데이터)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train_pred, train_residuals, alpha=0.5, label='Train Residuals')\n",
    "plt.hlines(0, min(y_train_pred), max(y_train_pred), color='black', linestyle='--')\n",
    "plt.title('Residuals vs Predicted (Train Data)')\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 잔차 플롯 (검증 데이터)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val_pred, val_residuals, alpha=0.5, label='Validation Residuals', color='red')\n",
    "plt.hlines(0, min(y_val_pred), max(y_val_pred), color='black', linestyle='--')\n",
    "plt.title('Residuals vs Predicted (Validation Data)')\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 오차 히스토그램 (훈련 데이터)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_residuals, bins=50, alpha=0.5, label='Train Residuals', color='blue')\n",
    "plt.title('Distribution of Prediction Errors (Train Data)')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 예측 오차 히스토그램 (검증 데이터)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(val_residuals, bins=50, alpha=0.5, label='Validation Residuals', color='red')\n",
    "plt.title('Distribution of Prediction Errors (Validation Data)')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 그래프 (훈련 데이터)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(len(y_train)), y_train, label='Actual Train Prices', color='blue')\n",
    "plt.plot(range(len(y_train_pred)), y_train_pred, label='Predicted Train Prices', color='red', linestyle='--')\n",
    "plt.title('Actual vs Predicted Prices on Training Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 시계열 그래프 (검증 데이터)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(len(y_val)), y_val, label='Actual Validation Prices', color='blue')\n",
    "plt.plot(range(len(y_val_pred)), y_val_pred, label='Predicted Validation Prices', color='red', linestyle='--')\n",
    "plt.title('Actual vs Predicted Prices on Validation Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
