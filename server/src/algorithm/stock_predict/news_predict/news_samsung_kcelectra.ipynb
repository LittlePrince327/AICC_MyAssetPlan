{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1fa0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from konlpy.tag import Komoran\n",
    "from pykospacing import Spacing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "import optuna\n",
    "import itertools\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348b9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정 : PyTorch를 사용할 때, GPU가 사용 가능한 경우 \"cuda\"를 사용하고, 그렇지 않으면 CPU(\"cpu\")를 사용하도록 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "komoran = Komoran()                 # Komoran 객체 생성: 한국어 형태소 분석기 Komoran을 초기화\n",
    "spacing = Spacing()                 # Spacing 객체 생성: 텍스트에서 띄어쓰기를 자동으로 교정해주는 Spacing 라이브러리를 초기화\n",
    "label_encoder = LabelEncoder()      # LabelEncoder 객체 생성: 범주형 데이터를 수치형 데이터로 변환하는 LabelEncoder를 초기화\n",
    "\n",
    "# tqdm과 pandas 통합: pandas의 프로그레스 바 기능을 tqdm과 통합하여 데이터프레임의 연산 시 진행 상황을 시각적으로 표시\n",
    "tqdm.pandas()\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽어오기: 지정된 경로에서 'samsung_news_fil_cleaned.xlsx' 엑셀 파일을 읽어와서 데이터프레임에 저장\n",
    "df = pd.read_excel(r'../../../data/samsung_news_fil_cleaned.xlsx')\n",
    "\n",
    "# 데이터의 일부만 가져오기 (처음 10000개 행): 전체 데이터가 클 경우 처음 10000개 행만 사용하도록 하는 코드\n",
    "# df = df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542f67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "Outcome\n",
      "호재    9345\n",
      "악재    3861\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 호재 악재 갯수 확인\n",
    "print(\"Class distribution:\")\n",
    "print(df['Outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f7965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome 레이블 변경\n",
    "# 'Outcome' : '호재'는 0으로, '악재'는 1로 변경하여 이진 분류 문제로 변환\n",
    "df.loc[(df['Outcome'] == '호재'), 'Outcome'] = 0\n",
    "df.loc[(df['Outcome'] == '악재'), 'Outcome'] = 1\n",
    "\n",
    "# 클래스별로 데이터 분리\n",
    "# 'Outcome' 값에 따라 두 개의 데이터프레임으로 분리\n",
    "df_positive = df[df['Outcome'] == 0]  # 0: 호재\n",
    "df_negative = df[df['Outcome'] == 1]  # 1: 악재\n",
    "\n",
    "# 최소 클래스의 샘플 수 확인\n",
    "# 더 적은 샘플 수를 가진 클래스를 찾아 그 개수를 저장\n",
    "min_class_count = min(len(df_positive), len(df_negative))\n",
    "\n",
    "# 최소 샘플 수 보장 및 계산\n",
    "# 샘플 수를 보정, 최소 클래스의 1.25배 및 1.35배가 되도록 설정\n",
    "positive_n_samples = max(int(min_class_count * 1.25), 1)\n",
    "negative_n_samples = max(int(min_class_count * 1.35), 1)\n",
    "\n",
    "# 다운샘플링 적용\n",
    "df_positive_downsampled = resample(df_positive,\n",
    "                                   replace=True,                    # 샘플을 복원하지 않고\n",
    "                                   n_samples=positive_n_samples,    # 최소 클래스의 개수에 비례해서 설정\n",
    "                                   random_state=42)                 # 재현성을 위해 random_state 사용\n",
    "\n",
    "df_negative_downsampled = resample(df_negative,\n",
    "                                   replace=True,                    # 샘플을 복원하지 않고\n",
    "                                   n_samples=negative_n_samples,    # 최소 클래스의 개수에 비례해서 설정\n",
    "                                   random_state=42)                 # 재현성을 위해 random_state 사용\n",
    "\n",
    "# 다운샘플링된 데이터 결합\n",
    "df_resampled = pd.concat([df_positive_downsampled, df_negative_downsampled])\n",
    "\n",
    "# 데이터프레임을 무작위로 셔플, 인덱스 초기화, 데이터의 순서를 랜덤 배치 \n",
    "df_resampled = df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 데이터를 학습용(train)과 평가용(eval)으로 8:2 비율로 분리\n",
    "train_df, eval_df = train_test_split(df_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# 학습 데이터의 텍스트와 라벨을 리스트로 변환\n",
    "train_texts     = train_df['cleaned'].tolist()\n",
    "train_labels    = train_df['Outcome'].tolist()\n",
    "\n",
    "# 평가 데이터의 텍스트와 라벨을 리스트로 변환\n",
    "eval_texts      = eval_df['cleaned'].tolist()\n",
    "eval_labels     = eval_df['Outcome'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6745411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset 클래스 정의 / 학습 및 평가에 사용할 데이터셋 정의\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        # 생성자에서 텍스트 데이터, 라벨, 토크나이저, 최대 길이를 초기화\n",
    "        self.texts      = texts                             # 텍스트 데이터 리스트\n",
    "        self.labels     = labels                            # 라벨 데이터 리스트\n",
    "        self.tokenizer  = tokenizer                         # 사용할 토크나이저\n",
    "        self.max_len    = max_len                           # 토큰화된 시퀀스의 최대 길이\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋 샘플 수 반환\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스 해당하는 텍스트와 라벨을 반환\n",
    "        text    = self.texts[idx]                           # 인덱스에 해당하는 텍스트\n",
    "        label   = self.labels[idx]                          # 인덱스에 해당하는 라벨\n",
    "\n",
    "        # 텍스트를 토크나이저로 인코딩, 모델 입력으로 변환\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,                                           # 인코딩할 텍스트\n",
    "            add_special_tokens      = True,                 # 문장의 시작과 끝에 특별 토큰 추가\n",
    "            max_length              = self.max_len,         # 최대 길이로 패딩 및 자르기 수행\n",
    "            return_token_type_ids   = False,                # token type ids 반환 안함\n",
    "            padding                 = 'max_length',         # max_length에 맞춰 패딩 추가\n",
    "            truncation              = True,                 # max_length를 초과하는 텍스트는 자름\n",
    "            return_attention_mask   = True,                 # attention mask 반환\n",
    "            return_tensors          = 'pt',                 # PyTorch 텐서로 반환\n",
    "        )\n",
    "\n",
    "        # 반환된 인코딩 결과를 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),               # 토큰 ID 텐서를 1차원으로 평탄화하여 반환\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),     # attention mask 텐서를 1차원으로 평탄화하여 반환\n",
    "            'labels': torch.tensor(label, dtype=torch.long)             # 라벨을 long 타입의 텐서로 변환하여 반환\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ea7643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ElectraTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# KcELECTRA 토크나이저 로드\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"beomi/KcELECTRA-base-v2022\")\n",
    "\n",
    "# 모델 초기화 함수 정의 / KcELECTRA 기반의 시퀀스 분류 모델 초기화\n",
    "def initialize_model():\n",
    "    # num_labels=2로 이진 분류를 수행하도록 설정 / 사전 학습된 KcELECTRA 모델 불러오기, 시퀀스 분류를 위해 사용\n",
    "    model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base-v2022\", num_labels=2)\n",
    "    \n",
    "    # 모델 GPU로 이동\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저로 AdamW 설정, 학습률은 5e-5 설정\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "# 평가지표를 계산하는 함수 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    # 예측 결과와 실제 라벨 분리\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # 예측 로짓에서 가장 높은값으로 최종 만듬\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    \n",
    "    # 예측값과 실제 라벨 비교 후 정확도 계산\n",
    "    acc = accuracy_score(labels, predictions.numpy())\n",
    "    \n",
    "    # precision, recall, f1 점수를 가중 평균 방식(average='weighted')으로 계산\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions.numpy(), average='weighted')\n",
    "    \n",
    "    # 계산된 지표들 딕셔너리 형태로 반환\n",
    "    return {\n",
    "        'accuracy'  : acc,\n",
    "        'f1'        : f1,\n",
    "        'precision' : precision,\n",
    "        'recall'    : recall\n",
    "    }\n",
    "\n",
    "# 전처리된 텍스트와 라벨로 학습 및 평가 데이터셋 인스턴스 생성\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_len=256)\n",
    "eval_dataset = NewsDataset(eval_texts, eval_labels, tokenizer, max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0f04edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그를 저장할 리스트 초기화\n",
    "log_list = []\n",
    "\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "parameter_grid = {\n",
    "    'learning_rate': [1e-5, 3e-5],  # 학습률: 적당히 넓은 범위에서 선택 \n",
    "    'batch_size': [16],             # 배치 크기\n",
    "    'num_train_epochs': [5],        # 학습 에폭 수\n",
    "    'weight_decay': [0.01, 0.005],  # 가중치 감소 , 과적합 방지를 위해 작은값 추가 \n",
    "    'warmup_steps': [500],          # 웜업 스텝수 \n",
    "}\n",
    "\n",
    "# 최적화된 하이퍼파라미터를 저장할 변수\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c38f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 그리드의 모든 조합 생성\n",
    "param_combinations = list(itertools.product(*parameter_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e566ae95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-05, 16, 5, 0.01, 500),\n",
       " (1e-05, 16, 5, 0.005, 500),\n",
       " (3e-05, 16, 5, 0.01, 500),\n",
       " (3e-05, 16, 5, 0.005, 500)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하이퍼파라미터 그리드 조합 확인\n",
    "param_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11cdf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: LR: 1e-05, Batch Size: 16, Epochs: 5, WD: 0.01, Warmup Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "  4%|▍         | 50/1255 [01:12<29:18,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6931, 'learning_rate': 5e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 100/1255 [02:25<27:55,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6936, 'learning_rate': 1e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 150/1255 [03:37<26:57,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6724, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 200/1255 [04:49<24:55,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6095, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 250/1255 [06:03<24:24,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5853, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 251/1255 [06:04<23:07,  1.38s/it]\n",
      " 20%|██        | 251/1255 [06:30<23:07,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5545030832290649, 'eval_accuracy': 0.6862549800796812, 'eval_f1': 0.656681350076372, 'eval_precision': 0.7593611869439794, 'eval_recall': 0.6862549800796812, 'eval_runtime': 26.1739, 'eval_samples_per_second': 76.718, 'eval_steps_per_second': 4.814, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 300/1255 [07:44<23:08,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5698, 'learning_rate': 3e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 350/1255 [08:58<22:12,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.604, 'learning_rate': 3.5e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 400/1255 [10:12<21:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5543, 'learning_rate': 4e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 450/1255 [11:27<19:40,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5975, 'learning_rate': 4.5e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 500/1255 [12:42<18:42,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5197, 'learning_rate': 5e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 502/1255 [12:44<17:35,  1.40s/it]\n",
      " 40%|████      | 502/1255 [13:11<17:35,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5106509923934937, 'eval_accuracy': 0.6937250996015937, 'eval_f1': 0.6567039977828358, 'eval_precision': 0.8071944151106418, 'eval_recall': 0.6937250996015937, 'eval_runtime': 26.678, 'eval_samples_per_second': 75.268, 'eval_steps_per_second': 4.723, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 550/1255 [14:26<17:20,  1.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5394, 'learning_rate': 4.668874172185431e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 600/1255 [15:39<15:50,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5381, 'learning_rate': 4.337748344370861e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 650/1255 [16:53<14:49,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5555, 'learning_rate': 4.006622516556292e-05, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 700/1255 [18:05<13:22,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5198, 'learning_rate': 3.675496688741722e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 750/1255 [19:17<12:13,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5054, 'learning_rate': 3.3443708609271526e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 753/1255 [19:21<11:31,  1.38s/it]\n",
      " 60%|██████    | 753/1255 [19:46<11:31,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5061969757080078, 'eval_accuracy': 0.6937250996015937, 'eval_f1': 0.6567039977828358, 'eval_precision': 0.8071944151106418, 'eval_recall': 0.6937250996015937, 'eval_runtime': 24.9548, 'eval_samples_per_second': 80.466, 'eval_steps_per_second': 5.049, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 800/1255 [20:58<10:59,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5072, 'learning_rate': 3.0132450331125826e-05, 'epoch': 3.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 850/1255 [22:11<09:48,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5203, 'learning_rate': 2.6821192052980134e-05, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 900/1255 [23:24<08:31,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5064, 'learning_rate': 2.3509933774834437e-05, 'epoch': 3.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 950/1255 [24:36<07:28,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5278, 'learning_rate': 2.0198675496688745e-05, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1000/1255 [25:52<06:15,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.499, 'learning_rate': 1.688741721854305e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1004/1255 [25:58<06:01,  1.44s/it]\n",
      " 80%|████████  | 1004/1255 [26:23<06:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5048550367355347, 'eval_accuracy': 0.6937250996015937, 'eval_f1': 0.6567039977828358, 'eval_precision': 0.8071944151106418, 'eval_recall': 0.6937250996015937, 'eval_runtime': 25.6317, 'eval_samples_per_second': 78.34, 'eval_steps_per_second': 4.916, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 1050/1255 [27:36<05:01,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5105, 'learning_rate': 1.3576158940397351e-05, 'epoch': 4.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1100/1255 [28:49<03:42,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5154, 'learning_rate': 1.0264900662251655e-05, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1150/1255 [30:01<02:31,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.508, 'learning_rate': 6.95364238410596e-06, 'epoch': 4.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1200/1255 [31:13<01:18,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5114, 'learning_rate': 3.642384105960265e-06, 'epoch': 4.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1250/1255 [32:25<00:07,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5042, 'learning_rate': 3.3112582781456954e-07, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1255/1255 [32:32<00:00,  1.36s/it]\n",
      "100%|██████████| 1255/1255 [32:58<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.504021942615509, 'eval_accuracy': 0.6937250996015937, 'eval_f1': 0.6567039977828358, 'eval_precision': 0.8071944151106418, 'eval_recall': 0.6937250996015937, 'eval_runtime': 25.8354, 'eval_samples_per_second': 77.723, 'eval_steps_per_second': 4.877, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\trainer.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
      "100%|██████████| 1255/1255 [33:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1983.019, 'train_samples_per_second': 20.247, 'train_steps_per_second': 0.633, 'train_loss': 0.5543638596021797, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:24<00:00,  5.05it/s]\n",
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Epoch 5 | LR: 1e-05 | Batch Size: 16 | WD: 0.01 | Warmup: 500\n",
      "Loss: 0.5040 | Acc: 0.6937 | F1: 0.6567 | Prec: 0.8072 | Rec: 0.6937\n",
      "--------------------------------------------------\n",
      "Testing combination: LR: 1e-05, Batch Size: 16, Epochs: 5, WD: 0.005, Warmup Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "  4%|▍         | 50/1255 [03:11<1:17:29,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6954, 'learning_rate': 5e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 100/1255 [06:22<1:13:41,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.693, 'learning_rate': 1e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 150/1255 [09:34<1:10:21,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6854, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 194/1255 [12:25<1:09:07,  3.91s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 42\u001b[0m\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     33\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(optimizer, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# 모델 평가\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1892\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1895\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1896\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1898\u001b[0m ):\n\u001b[0;32m   1899\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\transformers\\trainer.py:2787\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\accelerate\\accelerator.py:2159\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2159\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dlavk\\anaconda3\\envs\\test_pytorch\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 그리드 서치 시작\n",
    "for param_set in param_combinations:\n",
    "    # 각 하이퍼파라미터 조합을 변수에 할당\n",
    "    learning_rate, batch_size, num_train_epochs, weight_decay, warmup_steps = param_set\n",
    "    \n",
    "    # 현재 조합에 대한 정보를 출력\n",
    "    print(f\"Testing combination: LR: {learning_rate}, Batch Size: {batch_size}, Epochs: {num_train_epochs}, WD: {weight_decay}, Warmup Steps: {warmup_steps}\")\n",
    "\n",
    "    # 모델과 옵티마이저 초기화 (사용자 정의 함수로 가정)\n",
    "    model, optimizer = initialize_model()\n",
    "\n",
    "    # TrainingArguments 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir                  = './results',          # 학습 결과를 저장할 디렉토리\n",
    "        num_train_epochs            = num_train_epochs,     # 학습 에폭 수\n",
    "        per_device_train_batch_size = batch_size,           # 훈련 배치 크기\n",
    "        per_device_eval_batch_size  = batch_size,           # 평가 배치 크기\n",
    "        warmup_steps                = warmup_steps,         # 웜업 스텝 수\n",
    "        weight_decay                = weight_decay,         # 가중치 감소 비율\n",
    "        logging_dir                 = './logs',             # 로그를 저장할 디렉토리\n",
    "        logging_steps               = 50,                   # 로그를 기록할 스텝 간격\n",
    "        evaluation_strategy         = \"epoch\",              # 에폭마다 평가 수행\n",
    "        save_total_limit            = 1,                    # 최대 1개의 체크포인트만 저장\n",
    "        save_strategy               = \"epoch\",              # 에폭마다 모델을 저장\n",
    "        load_best_model_at_end      = True,                 # 학습 종료 시 가장 좋은 모델을 로드\n",
    "        gradient_accumulation_steps = 2,                    # 그래디언트 누적 스텝 수\n",
    "        no_cuda                     = False,                # CUDA 사용 여부 (False로 설정하면 GPU를 사용)\n",
    "        learning_rate               = learning_rate,        # 학습률\n",
    "        report_to                   = \"none\",               # 로깅을 어디로 보낼지 설정 (none이면 로깅하지 않음)\n",
    "    )\n",
    "\n",
    "    # Trainer 설정\n",
    "    trainer = Trainer(\n",
    "        model                       = model,                # 학습할 모델\n",
    "        args                        = training_args,        # 학습 설정\n",
    "        train_dataset               = train_dataset,        # 훈련 데이터셋\n",
    "        eval_dataset                = eval_dataset,         # 평가 데이터셋\n",
    "        compute_metrics             = compute_metrics,      # 평가지표 계산 함수\n",
    "        optimizers                  = (optimizer, None)     # 옵티마이저 설정\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 모델 학습\n",
    "        trainer.train()\n",
    "\n",
    "        # 모델 평가\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # 평가 지표를 로그 리스트에 저장 /  조합 평가 결과를 기록\n",
    "        log_list.append({\n",
    "            'epoch'         : num_train_epochs,\n",
    "            'learning_rate' : learning_rate,\n",
    "            'batch_size'    : batch_size,\n",
    "            'weight_decay'  : weight_decay,\n",
    "            'warmup_steps'  : warmup_steps,\n",
    "            'eval_loss'     : eval_results.get('eval_loss'),\n",
    "            'eval_accuracy' : eval_results.get('eval_accuracy'),\n",
    "            'eval_f1'       : eval_results.get('eval_f1'),\n",
    "            'eval_precision': eval_results.get('eval_precision'),\n",
    "            'eval_recall'   : eval_results.get('eval_recall')\n",
    "        })\n",
    "\n",
    "        # 현재 에폭과 평가 결과 출력\n",
    "        print(f\"Results:\")\n",
    "        print(f\"Epoch {num_train_epochs} | LR: {learning_rate} | Batch Size: {batch_size} | WD: {weight_decay} | Warmup: {warmup_steps}\")\n",
    "        print(f\"Loss: {eval_results['eval_loss']:.4f} | Acc: {eval_results['eval_accuracy']:.4f} | F1: {eval_results['eval_f1']:.4f} | Prec: {eval_results['eval_precision']:.4f} | Rec: {eval_results['eval_recall']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # 최적의 하이퍼파라미터 및 모델 업데이트\n",
    "        if eval_results['eval_accuracy'] > best_accuracy:\n",
    "            best_accuracy = eval_results['eval_accuracy']\n",
    "            best_params = {\n",
    "                'learning_rate'     : learning_rate,\n",
    "                'batch_size'        : batch_size,\n",
    "                'num_train_epochs'  : num_train_epochs,\n",
    "                'weight_decay'      : weight_decay,\n",
    "                'warmup_steps'      : warmup_steps\n",
    "            }\n",
    "            best_model = model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# 로그를 DataFrame으로 변환하여 출력\n",
    "df_logs = pd.DataFrame(log_list)\n",
    "print(df_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6e23fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [01:46<00:00,  4.71it/s]\n",
      "100%|██████████| 126/126 [00:26<00:00,  4.75it/s]\n",
      "100%|██████████| 126/126 [00:25<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6909090909090909\n",
      "Validation Accuracy: 0.6967131474103586\n",
      "\n",
      "Accuracy on Validation Set: 0.6967131474103586\n",
      "Confusion Matrix on Validation Set:\n",
      " [[ 386  577]\n",
      " [  32 1013]]\n",
      "Classification Report on Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.40      0.56       963\n",
      "           1       0.64      0.97      0.77      1045\n",
      "\n",
      "    accuracy                           0.70      2008\n",
      "   macro avg       0.78      0.69      0.66      2008\n",
      "weighted avg       0.77      0.70      0.67      2008\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAK7CAYAAADWX59rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABBFElEQVR4nO3dfbzX8/348eeni3O6TtfpuiRyWUKKFJFviDCEUeTaNkJ8m69iNtFsriZXUYiwuZhQm83FjBArlzGNVKpJKpPK6Zz37w+/zhyncg6nTrzu99ut23be7/fn/Xm+P+eY89j7olyWZVkAAAAkrEplDwAAAFDZhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQR8L3x2muvxYknnhjt27ePGjVqRJ06dWKXXXaJMWPGxCeffLJR33vGjBnRu3fvqF+/fuRyubjmmmsq/D1yuVxccsklFb7fbzJhwoTI5XKRy+Xi6aefLrU+y7Lo2LFj5HK56NOnz7d6j7Fjx8aECRPK9Zqnn356vTNtrrp27RotW7aMwsLC9W6z5557RuPGjeOLL74o0z7nzJkTuVyuxOe39ns2Z86cb3x9nz59vvX37fLLL4+HH3641PLK/N5kWRb33ntv9OrVK5o2bRo1atSIVq1axQEHHBDjxo37Vvv8Nj+fwA+PMAK+F2699dbo1q1bTJ8+PYYPHx5Tp06Nhx56KI488si46aabYujQoRv1/U866aRYuHBh3HvvvTFt2rQYNGhQhb/HtGnT4uSTT67w/ZZV3bp147bbbiu1/Jlnnol//etfUbdu3W+972/zi+cuu+wS06ZNi1122eVbv++mNnTo0FiwYEH86U9/Wuf6f/7zn/H888/H8ccfH3l5ed/6fQ466KCYNm1abLnllt96H2WxvjCqzO/NiBEj4phjjonOnTvHuHHjYsqUKfHLX/4ymjVrFn/84x+/1T6FERARUa2yBwD4JtOmTYszzjgj9t9//3j44YcjPz+/eN3+++8f5513XkydOnWjzvDGG2/EKaecEv37999o77HHHntstH2XxdFHHx1333133HDDDVGvXr3i5bfddlv06NEjPv30000yR0FBQeRyuahXr16lfyblddxxx8Xw4cPj9ttvjwMPPLDU+ttvvz0ivgzt76JJkybRpEmT77SP76KyvjcrV66Ma665Jk444YS45ZZbSqwbMmRIFBUVbfKZgB8OZ4yAzd7ll18euVwubrnllhJRtFZeXl4ccsghxV8XFRXFmDFjYtttt438/Pxo2rRpnHDCCTF//vwSr+vTp0/ssMMOMX369OjVq1fUqlUrOnToEFdccUXxL1hrL1las2ZN3HjjjcWXnEVEXHLJJcX//avWdZnTk08+GX369IlGjRpFzZo1o02bNnHEEUfE559/XrzNui6le+ONN+LQQw+NBg0aRI0aNaJLly5xxx13lNhm7WVNkyZNiosuuihatGgR9erVi/322y/eeeedsn3IEXHMMcdERMSkSZOKly1fvjweeOCB9f4if+mll0b37t2jYcOGUa9evdhll13itttuiyzLirdp165dvPnmm/HMM88Uf37t2rUrMftdd90V5513XrRs2TLy8/Nj9uzZpS7X+vjjj6N169bRs2fPKCgoKN7/W2+9FbVr147jjz/+G4/x73//e/Tt2zfq1q0btWrVip49e8Zjjz1WYpu137+nnnoqzjjjjGjcuHE0atQoDj/88FiwYMEG99+gQYM47LDDYvLkybFkyZIS6woLC+Ouu+6K3XbbLXbccceYPXt2nHjiibH11ltHrVq1omXLljFgwIB4/fXXv/E41vUzlmVZjBkzJtq2bRs1atSIXXbZJaZMmVLqtatWrYrzzjsvunTpEvXr14+GDRtGjx49Sp1tyeVysWLFirjjjjuKv29rL8lb36V0jzzySPTo0SNq1aoVdevWjf333z+mTZtWYpu1/9y8+eabccwxx0T9+vWjWbNmcdJJJ8Xy5cs3eNwrVqyI1atXr/dMWZUqJX+t+eKLL+KXv/xl8f8WNGnSJE488cRYvHhx8TYb+vkE0iKMgM1aYWFhPPnkk9GtW7do3bp1mV5zxhlnxIUXXhj7779/PPLII3HZZZfF1KlTo2fPnvHxxx+X2HbRokVx3HHHxY9//ON45JFHon///jFixIiYOHFiRPz3kqWIiB/96Ecxbdq0Ur/ofZM5c+bEQQcdFHl5eXH77bfH1KlT44orrojatWtv8D6Td955J3r27BlvvvlmXHfddfHggw/GdtttF0OGDIkxY8aU2v7nP/95fPDBBzFu3Li45ZZb4t13340BAwZs8H6Xr6pXr1786Ec/Kj6rEfFlJFWpUiWOPvro9R7baaedFvfff388+OCDcfjhh8dPf/rTuOyyy4q3eeihh6JDhw7RtWvX4s/voYceKrGfESNGxNy5c+Omm26KyZMnR9OmTUu9V+PGjePee++N6dOnx4UXXhgREZ9//nkceeSR0aZNm7jppps2eHzPPPNM7LvvvrF8+fK47bbbYtKkSVG3bt0YMGBA3HfffaW2P/nkk6N69epxzz33xJgxY+Lpp5+OH//4xxt8j4gvL6f74osvin+G1vrTn/4UCxYsKL7sc8GCBdGoUaO44oorYurUqXHDDTdEtWrVonv37uUK2rUuvfTS4p/7hx9+OM4444w45ZRTSu1r9erV8cknn8T5558fDz/8cEyaNCn22muvOPzww+POO+8s3m7atGlRs2bNOPDAA4u/b2PHjl3v+99zzz1x6KGHRr169WLSpElx2223xdKlS6NPnz7x97//vdT2RxxxRHTq1CkeeOCB+N///d+45557YtiwYRs8xsaNG0fHjh1j7Nix8dvf/jbefvvtEhH+VUVFRXHooYfGFVdcEccee2w89thjccUVV8QTTzwRffr0iZUrV0ZE2X4+gURkAJuxRYsWZRGRDRo0qEzbz5o1K4uI7Mwzzyyx/MUXX8wiIvv5z39evKx3795ZRGQvvvhiiW2322677IADDiixLCKys846q8SyUaNGZev6n9Hx48dnEZG9//77WZZl2R/+8IcsIrKZM2ducPaIyEaNGlX89aBBg7L8/Pxs7ty5Jbbr379/VqtWrWzZsmVZlmXZU089lUVEduCBB5bY7v77788iIps2bdoG33ftvNOnTy/e1xtvvJFlWZbttttu2ZAhQ7Isy7Ltt98+692793r3U1hYmBUUFGS/+MUvskaNGmVFRUXF69b32rXvt/fee6933VNPPVVi+ZVXXplFRPbQQw9lgwcPzmrWrJm99tprGzzGLMuyPfbYI2vatGn2n//8p3jZmjVrsh122CFr1apV8bxrP4+v/wyNGTMmi4hs4cKFG3yfoqKirH379tlOO+1UYvkRRxyR1apVK1u+fPk6X7dmzZrsiy++yLbeeuts2LBhxcvff//9LCKy8ePHFy/7+s/Y0qVLsxo1amSHHXZYiX0+99xzWURs8Pu2Zs2arKCgIBs6dGjWtWvXEutq166dDR48uNRrvv69KSwszFq0aJHtuOOOWWFhYfF2//nPf7KmTZtmPXv2LF629p+bMWPGlNjnmWeemdWoUaPEz826vPTSS1mbNm2yiMgiIqtbt2528MEHZ3feeWeJ106aNCmLiOyBBx4o8frp06dnEZGNHTu2eNk3/WwDaXDGCPhBeeqppyLiy/sNvmr33XePzp07x1//+tcSy5s3bx677757iWU77bRTfPDBBxU2U5cuXSIvLy9OPfXUuOOOO+K9994r0+uefPLJ6Nu3b6kzZUOGDInPP/+81Jmrr15OGPHlcUREuY6ld+/esdVWW8Xtt98er7/+ekyfPn2D98M8+eSTsd9++0X9+vWjatWqUb169Rg5cmQsWbIkPvroozK/7xFHHFHmbYcPHx4HHXRQHHPMMXHHHXfE9ddfHzvuuOMGX7NixYp48cUX40c/+lHUqVOneHnVqlXj+OOPj/nz55c6s/JtP89cLhcnnnhivPbaa/HKK69ERMSSJUti8uTJccQRRxTfv7VmzZq4/PLLY7vttou8vLyoVq1a5OXlxbvvvhuzZs0q24fx/02bNi1WrVoVxx13XInlPXv2jLZt25ba/ve//33sueeeUadOnahWrVpUr149brvttnK/71rvvPNOLFiwII4//vgSl7PVqVMnjjjiiHjhhRdKXDYase7Pd9WqVd/4c7PbbrvF7NmzY+rUqfHzn/88evToEX/961/jhBNOiEMOOaT4DNKjjz4aW2yxRQwYMCDWrFlT/KdLly7RvHnz79XTDoFNQxgBm7XGjRtHrVq14v333y/T9mvv61jXPQgtWrQodd9Ho0aNSm2Xn59ffJlNRdhqq63iL3/5SzRt2jTOOuus2GqrrWKrrbaKa6+9doOvW7JkyXqPY+36r/r6say9H6s8x7L2l/qJEyfGTTfdFJ06dYpevXqtc9uXXnop+vXrFxFfPjXwueeei+nTp8dFF11U7vctz9PVcrlcDBkyJFatWhXNmzcv071FS5cujSzLNtnneeKJJ0aVKlVi/PjxERFx9913xxdffFHi6YnnnntuXHzxxTFw4MCYPHlyvPjiizF9+vTYeeedy/3zt3b25s2bl1r39WUPPvhgHHXUUdGyZcuYOHFiTJs2rTiAV61aVa73/fr7r+/zLSoqiqVLl5ZY/l0+3+rVq8cBBxwQv/rVr+JPf/pTzJs3L/r06ROPPvpo8X1V//73v2PZsmWRl5cX1atXL/Fn0aJFpS6rBfBUOmCzVrVq1ejbt29MmTIl5s+fH61atdrg9mt/2Vq4cGGpbRcsWBCNGzeusNlq1KgREV/es/HVh0Ks6xeuXr16Ra9evaKwsDBefvnluP766+Occ86JZs2arffR340aNYqFCxeWWr72AQAVeSxfNWTIkBg5cmTcdNNN8atf/Wq92917771RvXr1ePTRR4s/i4hY5+Odv8m6HmKxPgsXLoyzzjorunTpEm+++Wacf/75cd11123wNQ0aNIgqVapsss+zVatW0a9fv7jnnnviN7/5TYwfPz46duwYe++9d/E2EydOjBNOOCEuv/zyEq/9+OOPY4sttijX+639uV+0aFGpdYsWLSrxMIGJEydG+/bt47777ivxua9evbpc77mu91/f51ulSpVo0KDBt95/Wd7/nHPOiaeffjreeOONOPDAA4sfmrG+J1Z+l8fPAz9MzhgBm70RI0ZElmVxyimnrPNhBQUFBTF58uSIiNh3330jIkrd+D59+vSYNWtW9O3bt8LmWvvL5muvvVZi+dpZ1qVq1arRvXv3uOGGGyIi4h//+Md6t+3bt288+eSTpZ6Eduedd0atWrU22uOSW7ZsGcOHD48BAwbE4MGD17tdLpeLatWqRdWqVYuXrVy5Mu66665S21bUWbjCwsI45phjIpfLxZQpU2L06NFx/fXXx4MPPrjB19WuXTu6d+8eDz74YIk5ioqKYuLEidGqVavo1KnTd57vq4YOHRpLly6NkSNHxsyZM+PEE08sESK5XK7UUxYfe+yx+PDDD8v9XnvssUfUqFEj7r777hLLn3/++VKX/uVyucjLyysxy6JFi9b5dwCV9fu2zTbbRMuWLeOee+4p8TCEFStWxAMPPFD8pLrvqqCgoNSZvbXWXga49gzgwQcfHEuWLInCwsLYddddS/3ZZpttil9b0WeJge8nZ4yAzV6PHj3ixhtvjDPPPDO6desWZ5xxRmy//fZRUFAQM2bMiFtuuSV22GGHGDBgQGyzzTZx6qmnxvXXXx9VqlSJ/v37x5w5c+Liiy+O1q1bf+NTr8rjwAMPjIYNG8bQoUPjF7/4RVSrVi0mTJgQ8+bNK7HdTTfdFE8++WQcdNBB0aZNm1i1alXxk9/222+/9e5/1KhR8eijj8Y+++wTI0eOjIYNG8bdd98djz32WIwZMybq169fYcfydVdcccU3bnPQQQfFb3/72zj22GPj1FNPjSVLlsRVV121zkeq77jjjnHvvffGfffdFx06dIgaNWp8431B6zJq1Kh49tln489//nM0b948zjvvvHjmmWdi6NCh0bVr12jfvv16Xzt69OjYf//9Y5999onzzz8/8vLyYuzYsfHGG2/EpEmTynXWqiwOOeSQaNy4cfz617+OqlWrlorMgw8+OCZMmBDbbrtt7LTTTvHKK6/Er3/96288K7ouDRo0iPPPPz9++ctfxsknnxxHHnlkzJs3Ly655JJSl9IdfPDB8eCDD8aZZ54ZP/rRj2LevHlx2WWXxZZbbhnvvvtuiW133HHHePrpp2Py5Mmx5ZZbRt26dUsExVpVqlSJMWPGxHHHHRcHH3xwnHbaabF69er49a9/HcuWLSvTz1NZLF++PNq1axdHHnlk7LffftG6dev47LPP4umnn45rr702OnfuHIcffnhERAwaNCjuvvvuOPDAA+Pss8+O3XffPapXrx7z58+Pp556Kg499NA47LDDio+zIn4+ge+5yn32A0DZzZw5Mxs8eHDWpk2bLC8vL6tdu3bWtWvXbOTIkdlHH31UvF1hYWF25ZVXZp06dcqqV6+eNW7cOPvxj3+czZs3r8T+evfunW2//fal3mfw4MFZ27ZtSyyLdTyVLsu+fEJWz549s9q1a2ctW7bMRo0alY0bN67EE8OmTZuWHXbYYVnbtm2z/Pz8rFGjRlnv3r2zRx55pNR7fPWpdFmWZa+//no2YMCArH79+lleXl628847l3g6WZb99wlhv//970ssX9fTzNblq0+l25B1Pbnr9ttvz7bZZpssPz8/69ChQzZ69OjstttuK3H8WZZlc+bMyfr165fVrVs3i4jiz3d9s3913donn/35z3/OqlSpUuozWrJkSdamTZtst912y1avXr3BY3j22WezfffdN6tdu3ZWs2bNbI899sgmT55cps9jfU/J25Bhw4at84mBWfblk+SGDh2aNW3aNKtVq1a21157Zc8++2zWu3fvEp9zWZ5Kl2VfPg1v9OjRWevWrbO8vLxsp512yiZPnlxqf1mWZVdccUXWrl27LD8/P+vcuXN26623rvMpizNnzsz23HPPrFatWiWebre+z+Lhhx/OunfvntWoUSOrXbt21rdv3+y5554rsc3a91m8eHGJ5es6pq9bvXp1dtVVV2X9+/fP2rRpk+Xn52c1atTIOnfunF1wwQXZkiVLSmxfUFCQXXXVVdnOO++c1ahRI6tTp0627bbbZqeddlr27rvvFm+3vp9PIC25LFvPXwAAAACQCPcYAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJC8apU9wMYw8ZX5lT0CAJuBx978uLJHAKCSTTqhS5m2c8YIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkVavsAYCyefmJR+KVvzwSyz7+d0RENGnZNvY+/Pjo2KV7RER8sWpl/HXSrfHOK8/Fyv98GvWbNI/dDzgsdt3/kBL7mf/PN+Op+2+PD//1dlSpWjWat+0Yx1w4Oqrn5W/yYwKgfI7YuXn8aOfmJZYtW1kQZ/z+zYiImHRCl3W+7u5XPoxH31wcjWvnxfVHbLfOba555v148YPlFTovfJ8II/ieqNewcew76JRo2LxFRES8+rc/x32/GRmnjL45mrZqF3++a2zMeWtmDDxzRGzRpHm899rL8fj4a6Nug0axza57RsSXUXTPlSNiz0OPiQOG/DSqVq0W/577r8jlcpV5aACUw7ylK+NXT/yr+OuiLCv+76ff/0aJbbu0rBen9mwdL/3/4Fny+ReltunbqVEM2L5pzPzwPxtxatj8CSP4nujUrWeJr/c9emi88pfJ8eG7b0XTVu1i/rtvxU69+kW77bpERMQufQ+OV/76aCx475/FYfTniTfGbgccFnseckzxfhpt2WqTHQMA311hFrF81Zp1rvv68m6t68dbiz6Ljz77IiIisnW8drc29WPanGWxek3RxhkYvifcYwTfQ0VFhfHG809GwepV0WrrLy+JaL3NDvHPf0yLTz9ZHFmWxZw3Z8Qni+bHVjvtGhERK5YvjQ9nz4ra9beI8aN+Gr89/Yi44xfDYu7br1fmoQBQTs3r5sXYH20f1x7WOX7aq200rZO3zu3q16gWXVvVi6dmL1nvvto3rBntGtba4DaQiko9YzR//vy48cYb4/nnn49FixZFLpeLZs2aRc+ePeP000+P1q1bV+Z4sNn599z3Yvyon8aagi8ir0bNOHLYpdGkVbuIiPifwT+JR2/9TVz7k0FRpWrVyOWqxMGnnBdttt0xIiKWfrQwIiL+9sAdsd+xp0ezdlvF688+ERMvHx6nXTnOmSOA74HZi1fEjc+tjIWfro76NavFYTs2j0v7bx3DH3k7PltdWGLbvbdqGKsKCmP6Bu4b2mfrRjF/2ap4d/HnG3t02OxVWhj9/e9/j/79+0fr1q2jX79+0a9fv8iyLD766KN4+OGH4/rrr48pU6bEnnvuucH9rF69OlavXl1iWcEXq91Izg9S4xat49TRt8Sqzz+LWS89G4/cdGWccPFvo0mrdvHS1Idi/uxZcfR5l0X9Js1i7qzXY8r4a6POFg2jw47dIvv/16Dvsu/B0aXP/0RExJbtto733/hHzHxmavQddHJlHhoAZfDqgv/eBzRvWcS7i9+Law7rHHt3aBiPz1pcYtveHRvGc+8vjYKiLNaletVc9GzfIB56bdHGHBm+NyotjIYNGxYnn3xyXH311etdf84558T06dM3uJ/Ro0fHpZdeWmLZYacMi8NPO7fCZoXNRdVq1aNh85YREdGiwzax8F/vxEtTH4x+J5wVT953Wxx17qWxddc9IiKiWZutYtEHs+OFx34fHXbsFnW2aBgREY1btS2xz8Yt28anH3+0aQ8EgAqxek1RzFu6KprXK/l/CG/TtHa0rF8jrvvbnPW+tnvbLSK/ai7+9q9PNvKU8P1QafcYvfHGG3H66aevd/1pp50Wb7zxxnrXrzVixIhYvnx5iT8DTjyrIkeFzVYWWaxZUxBFa9ZEUeGaUk+Xq1KlSmTZlzfTbtGkedRt0CiWLJhfYpslC+dH/cZNN9nMAFScalVy0aJ+fixbWVBi+T4dG8V7H38ec5euWu9r9+nYKF6Z/2n852uX4EGqKi2Mttxyy3j++efXu37atGmx5ZZbfuN+8vPzo169eiX+uIyOH6In7x0Xc99+LZYtXhT/nvtePHnfbfHBW6/Gjnv2jfxataNt553jL/fcEnPemhlLP1oYrz4zNV579onYZte9IiIil8tFj4OPjul/eijeevGZ+GTRh/HU/eNjyYK50WWfAyv56AAoi+O6tYjOzWpHkzp5sVXjWnFO73ZRs3rVEmd9alavEt3b1t/gAxWa1c2LbZvVjqfe9dAFWKvSLqU7//zz4/TTT49XXnkl9t9//2jWrFnkcrlYtGhRPPHEEzFu3Li45pprKms82Oys+HRpPDz2ivhs2SeRX6t2NGvdIY7939HRYccvnzp3+E//L568d1w8fMPlsfKz/0T9xs1in6NOim77DSjeR/f+R8Sagi/iibtujJUr/hPN2nSI40aMiYbNWlTWYQFQDg1rVY+f9moXdfOrxqer18S7iz+PkVP+GR+v+O8Zox7tGkQul4vn3l+63v306dgoln5eEK8t8HcXwVq5LMvWfUfeJnDffffF1VdfHa+88koUFn55Grdq1arRrVu3OPfcc+Ooo476Vvud+Mr8b94IgB+8x978uLJHAKCSTTqhS5m2q9THdR999NFx9NFHR0FBQXz88Zf/8mrcuHFUr169MscCAAASU6lhtFb16tXLdD8RAADAxlBpD18AAADYXAgjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5FVIGC1btqwidgMAAFApyh1GV155Zdx3333FXx911FHRqFGjaNmyZbz66qsVOhwAAMCmUO4wuvnmm6N169YREfHEE0/EE088EVOmTIn+/fvH8OHDK3xAAACAja1aeV+wcOHC4jB69NFH46ijjop+/fpFu3btonv37hU+IAAAwMZW7jNGDRo0iHnz5kVExNSpU2O//faLiIgsy6KwsLBipwMAANgEyn3G6PDDD49jjz02tt5661iyZEn0798/IiJmzpwZHTt2rPABAQAANrZyh9HVV18d7dq1i3nz5sWYMWOiTp06EfHlJXZnnnlmhQ8IAACwseWyLMsqe4iKNvGV+ZU9AgCbgcfe/LiyRwCgkk06oUuZtivTGaNHHnmkzG98yCGHlHlbAACAzUGZwmjgwIFl2lkul/MABgAA4HunTGFUVFS0secAAACoNOV+XPdXrVq1qqLmAAAAqDTlDqPCwsK47LLLomXLllGnTp147733IiLi4osvjttuu63CBwQAANjYyh1Gv/rVr2LChAkxZsyYyMvLK16+4447xrhx4yp0OAAAgE2h3GF05513xi233BLHHXdcVK1atXj5TjvtFG+//XaFDgcAALAplDuMPvzww+jYsWOp5UVFRVFQUFAhQwEAAGxK5Q6j7bffPp599tlSy3//+99H165dK2QoAACATalMj+v+qlGjRsXxxx8fH374YRQVFcWDDz4Y77zzTtx5553x6KOPbowZAQAANqpynzEaMGBA3HffffH4449HLpeLkSNHxqxZs2Ly5Mmx//77b4wZAQAANqpynzGKiDjggAPigAMOqOhZAAAAKsW3CqOIiJdffjlmzZoVuVwuOnfuHN26davIuQAAADaZcofR/Pnz45hjjonnnnsutthii4iIWLZsWfTs2TMmTZoUrVu3rugZAQAANqpy32N00kknRUFBQcyaNSs++eST+OSTT2LWrFmRZVkMHTp0Y8wIAACwUZX7jNGzzz4bzz//fGyzzTbFy7bZZpu4/vrrY88996zQ4QAAADaFcp8xatOmzTr/Itc1a9ZEy5YtK2QoAACATancYTRmzJj46U9/Gi+//HJkWRYRXz6I4eyzz46rrrqqwgcEAADY2HLZ2rrZgAYNGkQulyv+esWKFbFmzZqoVu3LK/HW/vfatWvHJ598svGmLaOJr8yv7BEA2Aw89ubHlT0CAJVs0gldyrRdme4xuuaaa77DKAAAAJu3MoXR4MGDN/YcAAAAleZb/wWvERErV64s9SCGevXqfaeBAAAANrVyP3xhxYoV8ZOf/CSaNm0aderUiQYNGpT4AwAA8H1T7jC64IIL4sknn4yxY8dGfn5+jBs3Li699NJo0aJF3HnnnRtjRgAAgI2q3JfSTZ48Oe68887o06dPnHTSSdGrV6/o2LFjtG3bNu6+++447rjjNsacAAAAG025zxh98skn0b59+4j48n6itY/n3muvveJvf/tbxU4HAACwCZQ7jDp06BBz5syJiIjtttsu7r///oj48kzSFltsUZGzAQAAbBLlDqMTTzwxXn311YiIGDFiRPG9RsOGDYvhw4dX+IAAAAAbWy7Lsuy77GDu3Lnx8ssvx1ZbbRU777xzRc31nUx8ZX5ljwDAZuCxNz+u7BEAqGSTTuhSpu3Kfcbo69q0aROHH354NGzYME466aTvujsAAIBN7jufMVrr1VdfjV122SUKCwsrYnffyao1lT0BAJuDBrv9pLJHAKCSrZzxuzJt953PGAEAAHzfCSMAACB5wggAAEhetbJuePjhh29w/bJly77rLAAAAJWizGFUv379b1x/wgknfOeBAAAANrUyh9H48eM35hwAAACVxj1GAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyvlUY3XXXXbHnnntGixYt4oMPPoiIiGuuuSb++Mc/VuhwAAAAm0K5w+jGG2+Mc889Nw488MBYtmxZFBYWRkTEFltsEddcc01FzwcAALDRlTuMrr/++rj11lvjoosuiqpVqxYv33XXXeP111+v0OEAAAA2hXKH0fvvvx9du3YttTw/Pz9WrFhRIUMBAABsSuUOo/bt28fMmTNLLZ8yZUpst912FTETAADAJlWtvC8YPnx4nHXWWbFq1arIsixeeumlmDRpUowePTrGjRu3MWYEAADYqModRieeeGKsWbMmLrjggvj888/j2GOPjZYtW8a1114bgwYN2hgzAgAAbFS5LMuyb/vijz/+OIqKiqJp06YVOdN3tmpNZU8AwOagwW4/qewRAKhkK2f8rkzblfuM0Vc1btz4u7wcAABgs1DuMGrfvn3kcrn1rn/vvfe+00AAAACbWrnD6JxzzinxdUFBQcyYMSOmTp0aw4cPr6i5AAAANplyh9HZZ5+9zuU33HBDvPzyy995IAAAgE2t3H+P0fr0798/HnjggYraHQAAwCZTYWH0hz/8IRo2bFhRuwMAANhkyn0pXdeuXUs8fCHLsli0aFEsXrw4xo4dW6HDAQAAbArlDqOBAweW+LpKlSrRpEmT6NOnT2y77bYVNRcAAMAmU64wWrNmTbRr1y4OOOCAaN68+caaCQAAYJMq1z1G1apVizPOOCNWr169seYBAADY5Mr98IXu3bvHjBkzNsYsAAAAlaLc9xideeaZcd5558X8+fOjW7duUbt27RLrd9pppwobDgAAYFPIZVmWlWXDk046Ka655prYYostSu8kl4ssyyKXy0VhYWFFz1huq9ZU9gQAbA4a7PaTyh4BgEq2csbvyrRdmcOoatWqsXDhwli5cuUGt2vbtm2Z3nhjEkYARAgjAMoeRmW+lG5tP20O4QMAAFCRyvXwha/+xa4AAAA/FOV6+EKnTp2+MY4++eST7zQQAADAplauMLr00kujfv36G2sWAACASlGuMBo0aFA0bdp0Y80CAABQKcp8j5H7iwAAgB+qModRGZ/qDQAA8L1T5kvpioqKNuYcAAAAlaZcj+sGAAD4IRJGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKqVfYAwLdz/733xP33TYoFH34YERFbddw6TjvjzNirV+8oKCiI3113Tfz92b/F/Pnzom6dOtG9R884e9h50bRps0qeHICy2nOXrWLYCfvFLtu1iS2b1I+jht0Sk59+rcQ2F512YAw9Ys/Yom7NmP7GB3HO6Pti1nuLitefdPiecXT/XaPLtq2iXp2a0bzX8Fj+2coS+/j9NafFzp1aRpOGdWPpp5/HUy++E/933R9j4eLlm+Q4YXPgjBF8TzVt1jzOHnZ+3HP/A3HP/Q/E7t33iLN/clbMnv1urFq1Kt6e9VacevoZcd/vH4zfXvu7+GDOnDj7J2dU9tgAlEPtmvnx+j8/jGFX3L/O9ecN2S9+9uN9YtgV98deP/51/HvJp/HYTT+NOrXyi7epVaN6PPH8W/Hr2/+83vf52/R/xo8vvD12PuwXcezwcdGhdeO459dDK/x4YHOWy7Isq+whKtqqNZU9AVSOXj12j2HnD4/Djziy1Lo3Xn8tjht0ZEx94qnYskWLSpgONr0Gu/2kskeACrNyxu9KnTF678+/ihvueSp+M+EvERGRV71afPDXy+P/rv1j3PbAcyVe36vb1vHncWev84zR1x3Ue8e4/7enRP3u58SaNUUVfzCwCa2c8bsybeeMEfwAFBYWxpTHH4uVKz+PnXfuus5tPvvss8jlclG3Xr1NPB0AG0O7lo1iyyb14y/T3i5e9kXBmnj2ldmxx84dvvV+G9SrFYP67xovvPq+KCIp7jGC77F3//lOHH/soPjii9VRq1atuPq6G2Krjh1Lbbd69eq49uqrov9BB0edOnUqYVIAKlrzxl/+H10fffKfEss/WvKfaLNlw3Lv75c/OzROH7R31K6ZHy++9n4c/rObKmRO+L7YrM8YzZs3L0466aQNbrN69er49NNPS/xZvXr1JpoQKle7du3j/gcejrvuuS+OPPqYuPjnF8a/Zs8usU1BQUFceP6wKCrK4qKLL6mcQQHYaL5+V0QuV3pZWVx9519ij0FXxkGn/y4KC4ti3GXHV9SI8L2wWYfRJ598EnfccccGtxk9enTUr1+/xJ9fXzl6E00Ilat6Xl60ads2tt9hxzh72HnRaZtt4+6JdxavLygoiOHnnRMfzp8fN4+73dkigB+QRR9/GhERzRqVvES6ScO6pc4ilcWSZSti9tyP4skX344T/nd89O+1Q3TfqX2FzArfB5V6Kd0jjzyywfXvvffeN+5jxIgRce6555ZYllXNX8/W8MOWZVkUfPFFRPw3iuZ+8EGMG39nbLFFg0qeDoCKNOfDJbFw8fLou8e28eo78yMionq1qtGrW8f4v2v/+J32nct9+Z951d11QToq9ad94MCBkcvlNni6N7f2n8z1yM/Pj/z8kiHkqXSk4Lprfht79do7mjVvHp+vWBFTpzweL09/KcbePC7WrFkT5w/7Wcya9VZcf8PNUVRYGB8vXhwREfXr14/qeXmVPD0AZVG7Zl5s1bpJ8dftWjaKnTq1jKWffh7zFi2NG+55KoYP7Rez534Us+cujguGHhArVxXEfVNeLn5Ns0Z1o1mjerFVm8YREbHD1i3iPytWxbxFS2Ppp5/Hrtu3jV13aBvPz/hXLPvP59GuZeMYecZB8a+5i+PF197f5McMlaVSH9fdsmXLuOGGG2LgwIHrXD9z5szo1q1bFBYWlmu/wogUjLr45/HSCy/E4sUfRZ26daNTp23ixKGnRI+ee8aHH86PA/v1Xefrxo2/M3bbvfsmnhYqh8d183239hHbX3fXIy/EqaMmRsR//4LXBvVqxfQ35sQ5o++Pt/61sHjbi047MP7v9ANL7eOUkXfFxMkvxvYdW8RVw4+IHTu1ito182LRx8vjz8/PiitvnRoL/AWv/ACU9XHdlRpGhxxySHTp0iV+8YtfrHP9q6++Gl27do2iovI9KlIYARAhjAAoexhV6qV0w4cPjxUrVqx3fceOHeOpp57ahBMBAAApqtQzRhuLM0YARDhjBEDZzxht1o/rBgAA2BSEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJE8YAQAAyRNGAABA8oQRAACQPGEEAAAkTxgBAADJE0YAAEDyhBEAAJA8YQQAACRPGAEAAMkTRgAAQPKEEQAAkDxhBAAAJC+XZVlW2UMAFWv16tUxevToGDFiROTn51f2OABUEv8+gLITRvAD9Omnn0b9+vVj+fLlUa9evcoeB4BK4t8HUHYupQMAAJInjAAAgOQJIwAAIHnCCH6A8vPzY9SoUW60BUicfx9A2Xn4AgAAkDxnjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieM4Adm7Nix0b59+6hRo0Z069Ytnn322coeCYBN7G9/+1sMGDAgWrRoEblcLh5++OHKHgk2e8IIfkDuu+++OOecc+Kiiy6KGTNmRK9evaJ///4xd+7cyh4NgE1oxYoVsfPOO8fvfve7yh4Fvjc8rht+QLp37x677LJL3HjjjcXLOnfuHAMHDozRo0dX4mQAVJZcLhcPPfRQDBw4sLJHgc2aM0bwA/HFF1/EK6+8Ev369SuxvF+/fvH8889X0lQAAN8Pwgh+ID7++OMoLCyMZs2alVjerFmzWLRoUSVNBQDw/SCM4Acml8uV+DrLslLLAAAoSRjBD0Tjxo2jatWqpc4OffTRR6XOIgEAUJIwgh+IvLy86NatWzzxxBMllj/xxBPRs2fPSpoKAOD7oVplDwBUnHPPPTeOP/742HXXXaNHjx5xyy23xNy5c+P000+v7NEA2IQ+++yzmD17dvHX77//fsycOTMaNmwYbdq0qcTJYPPlcd3wAzN27NgYM2ZMLFy4MHbYYYe4+uqrY++9967ssQDYhJ5++unYZ599Si0fPHhwTJgwYdMPBN8DwggAAEiee4wAAIDkCSMAACB5wggAAEieMAIAAJInjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMANjoLrnkkujSpUvx10OGDImBAwdu8jnmzJkTuVwuZs6cudHe4+vH+m1sijkBKEkYASRqyJAhkcvlIpfLRfXq1aNDhw5x/vnnx4oVKzb6e1977bUxYcKEMm27qSOhT58+cc4552yS9wJg81GtsgcAoPL8z//8T4wfPz4KCgri2WefjZNPPjlWrFgRN954Y6ltCwoKonr16hXyvvXr16+Q/QBARXHGCCBh+fn50bx582jdunUce+yxcdxxx8XDDz8cEf+9JOz222+PDh06RH5+fmRZFsuXL49TTz01mjZtGvXq1Yt99903Xn311RL7veKKK6JZs2ZRt27dGDp0aKxatarE+q9fSldUVBRXXnlldOzYMfLz86NNmzbxq1/9KiIi2rdvHxERXbt2jVwuF3369Cl+3fjx46Nz585Ro0aN2HbbbWPs2LEl3uell16Krl27Ro0aNWLXXXeNGTNmfOfP7MILL4xOnTpFrVq1okOHDnHxxRdHQUFBqe1uvvnmaN26ddSqVSuOPPLIWLZsWYn13zT7Vy1dujSOO+64aNKkSdSsWTO23nrrGD9+/Hc+FgD+yxkjAIrVrFmzxC/5s2fPjvvvvz8eeOCBqFq1akREHHTQQdGwYcN4/PHHo379+nHzzTdH375945///Gc0bNgw7r///hg1alTccMMN0atXr7jrrrviuuuuiw4dOqz3fUeMGBG33nprXH311bHXXnvFwoUL4+23346IL+Nm9913j7/85S+x/fbbR15eXkRE3HrrrTFq1Kj43e9+F127do0ZM2bEKaecErVr147BgwfHihUr4uCDD4599903Jk6cGO+//36cffbZ3/kzqlu3bkyYMCFatGgRr7/+epxyyilRt27duOCCC0p9bpMnT45PP/00hg4dGmeddVbcfffdZZr96y6++OJ46623YsqUKdG4ceOYPXt2rFy58jsfCwBfkQGQpMGDB2eHHnpo8dcvvvhi1qhRo+yoo47KsizLRo0alVWvXj376KOPirf561//mtWrVy9btWpViX1ttdVW2c0335xlWZb16NEjO/3000us7969e7bzzjuv870//fTTLD8/P7v11lvXOef777+fRUQ2Y8aMEstbt26d3XPPPSWWXXbZZVmPHj2yLMuym2++OWvYsGG2YsWK4vU33njjOvf1Vb17987OPvvs9a7/ujFjxmTdunUr/nrUqFFZ1apVs3nz5hUvmzJlSlalSpVs4cKFZZr968c8YMCA7MQTTyzzTACUnzNGAAl79NFHo06dOrFmzZooKCiIQw89NK6//vri9W3bto0mTZoUf/3KK6/EZ599Fo0aNSqxn5UrV8a//vWviIiYNWtWnH766SXW9+jRI5566ql1zjBr1qxYvXp19O3bt8xzL168OObNmxdDhw6NU045pXj5mjVriu9fmjVrVuy8885Rq1atEnN8V3/4wx/immuuidmzZ8dnn30Wa9asiXr16pXYpk2bNtGqVasS71tUVBTvvPNOVK1a9Rtn/7ozzjgjjjjiiPjHP/4R/fr1i4EDB0bPnj2/87EA8F/CCCBh++yzT9x4441RvXr1aNGiRamHK9SuXbvE10VFRbHlllvG008/XWpfW2yxxbeaoWbNmuV+TVFRUUR8eUla9+7dS6xbe8lflmXfap4NeeGFF2LQoEFx6aWXxgEHHBD169ePe++9N37zm99s8HW5XK74P8sy+9f1798/Pvjgg3jsscfiL3/5S/Tt2zfOOuusuOqqqyrgqACIEEYASatdu3Z07NixzNvvsssusWjRoqhWrVq0a9dundt07tw5XnjhhTjhhBOKl73wwgvr3efWW28dNWvWjL/+9a9x8sknl1q/9p6iwsLC4mXNmjWLli1bxnvvvRfHHXfcOve73XbbxV133RUrV64sjq8NzVEWzz33XLRt2zYuuuii4mUffPBBqe3mzp0bCxYsiBYtWkRExLRp06JKlSrRqVOnMs2+Lk2aNIkhQ4bEkCFDolevXjF8+HBhBFCBhBEAZbbffvtFjx49YuDAgXHllVfGNttsEwsWLIjHH388Bg4cGLvuumucffbZMXjw4Nh1111jr732irvvvjvefPPN9T58oUaNGnHhhRfGBRdcEHl5ebHnnnvG4sWL480334yhQ4dG06ZNo2bNmjF16tRo1apV1KhRI+rXrx+XXHJJ/OxnP4t69epF//79Y/Xq1fHyyy/H0qVL49xzz41jjz02Lrroohg6dGj83//9X8yZM6fMIbF48eJSf29S8+bNo2PHjjF37ty49957Y7fddovHHnssHnrooXUe0+DBg+Oqq66KTz/9NH72s5/FUUcdFc2bN4+I+MbZv27kyJHRrVu32H777WP16tXx6KOPRufOnct0LACUjcd1A1BmuVwuHn/88dh7773jpJNOik6dOsWgQYNizpw50axZs4iIOProo2PkyJFx4YUXRrdu3eKDDz6IM844Y4P7vfjii+O8886LkSNHRufOnePoo4+Ojz76KCIiqlWrFtddd13cfPPN0aJFizj00EMjIuLkk0+OcePGxYQJE2LHHXeM3r17x4QJE4of712nTp2YPHlyvPXWW9G1a9e46KKL4sorryzTcd5zzz3RtWvXEn9uuummOPTQQ2PYsGHxk5/8JLp06RLPP/98XHzxxaVe37Fjxzj88MPjwAMPjH79+sUOO+xQ4nHc3zT71+Xl5cWIESNip512ir333juqVq0a9957b5mOBYCyyWUb4yJsAACA7xFnjAAAgOQJIwAAIHnCCAAASJ4wAgAAkieMAACA5AkjAAAgecIIAABInjACAACSJ4wAAIDkCSMAACB5wggAAEje/wPxXX9hP4mDjgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model may be underfitting.\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터에 대한 평가\n",
    "train_results = trainer.evaluate(train_dataset)\n",
    "\n",
    "# 검증 데이터에 대한 평가\n",
    "eval_results = trainer.evaluate(eval_dataset)\n",
    "\n",
    "# 검증 데이터에 대한 예측 수행\n",
    "y_pred = trainer.predict(eval_dataset).predictions.argmax(-1)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"Training Accuracy:\", train_results['eval_accuracy'])\n",
    "print(\"Validation Accuracy:\", eval_results['eval_accuracy'])\n",
    "\n",
    "# 검증 데이터에서의 정확도, 혼동행렬, 분류 보고서를 출력\n",
    "print(\"\\nAccuracy on Validation Set:\", accuracy_score(eval_labels, y_pred))\n",
    "print(\"Confusion Matrix on Validation Set:\\n\", confusion_matrix(eval_labels, y_pred))\n",
    "print(\"Classification Report on Validation Set:\\n\", classification_report(eval_labels, y_pred))\n",
    "\n",
    "# 혼동행렬 계산 및 시각화\n",
    "conf_matrix = confusion_matrix(eval_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))  \n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)           # 혼동행렬을 히트맵으로 시각화\n",
    "plt.title('Confusion Matrix on Validation Set')\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')  \n",
    "plt.show()  \n",
    "\n",
    "# 훈련 데이터와 검증 데이터의 정확도를 비교하여 과대적합/과소적합 판별\n",
    "train_accuracy = train_results['eval_accuracy']                                   # 훈련 데이터에서의 정확도\n",
    "val_accuracy = eval_results['eval_accuracy']                                      # 검증 데이터에서의 정확도\n",
    "\n",
    "# 훈련 데이터의 정확도가 검증 데이터보다 현저히 높을 경우 과대적합 가능성을 제시\n",
    "if train_accuracy > val_accuracy and (train_accuracy - val_accuracy) > 0.05:\n",
    "    print(\"The model may be overfitting.\")\n",
    "# 훈련 데이터의 정확도가 검증 데이터보다 낮을 경우 과소적합 가능성을 제시\n",
    "elif train_accuracy < val_accuracy:\n",
    "    print(\"The model may be underfitting.\")\n",
    "# 훈련 데이터와 검증 데이터의 정확도가 유사할 경우 모델이 적절하게 학습된 것으로 판단\n",
    "else:\n",
    "    print(\"The model seems to have a good fit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90a7fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch  learning_rate  batch_size  weight_decay  warmup_steps  eval_loss  \\\n",
      "0      5        0.00003          16         0.001           500   0.502946   \n",
      "\n",
      "   eval_accuracy   eval_f1  eval_precision  eval_recall  \n",
      "0       0.696713  0.668233        0.774429     0.696713  \n",
      "   epoch  learning_rate  batch_size  weight_decay  warmup_steps  eval_loss  \\\n",
      "0      5        0.00003          16         0.001           500   0.502946   \n",
      "\n",
      "   eval_accuracy   eval_f1  eval_precision  eval_recall  \n",
      "0       0.696713  0.668233        0.774429     0.696713  \n"
     ]
    }
   ],
   "source": [
    "# 모든 결과 출력\n",
    "print(df_logs)\n",
    "df_logs_sorted = df_logs.sort_values(by='eval_accuracy', ascending=False)\n",
    "\n",
    "# 정렬된 결과 출력 (상위 5개만 예시로 출력)\n",
    "print(df_logs_sorted.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a792005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model may be underfitting.\n"
     ]
    }
   ],
   "source": [
    "# 과대적합/과소적합 판별\n",
    "train_accuracy = train_results['eval_accuracy']                                     # 훈련 데이터에 대한 정확도 저장\n",
    "val_accuracy = eval_results['eval_accuracy']                                        # 검증 데이터에 대한 정확도 저장\n",
    "\n",
    "# 훈련 정확도가 검증 정확도보다 높고, 두 정확도 간의 차이가 5% 이상일 때 과대적합 가능성 판단\n",
    "if train_accuracy > val_accuracy and (train_accuracy - val_accuracy) > 0.05:\n",
    "    print(\"The model may be overfitting.\")                                          # 모델이 과대적합일 가능성이 있음\n",
    "\n",
    "# 검증 정확도가 훈련 정확도보다 높을 때 과소적합 가능성 판단\n",
    "elif train_accuracy < val_accuracy:\n",
    "    print(\"The model may be underfitting.\")                                         # 모델이 과소적합일 가능성이 있음\n",
    "\n",
    "# 두 정확도 간의 차이가 크지 않을 때 모델이 적절하게 학습되었음을 의미\n",
    "else:\n",
    "    print(\"The model seems to have a good fit.\")                                    # 모델이 잘 맞는 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45597dd0-0c1c-4574-84e5-4e56e8db3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인덱스와 실제 라벨명을 매핑한 딕셔너리\n",
    "label_map = {0: '호재', 1: '악재'}                                      # 0은 '호재', 1은 '악재'로 매핑\n",
    "\n",
    "# 텍스트를 입력 받아 예측하는 함수 정의\n",
    "def predict_text(text, model, tokenizer, max_len):\n",
    "    \n",
    "    # 텍스트를 토크나이즈하고 인코딩하여 모델 입력 형식으로 변환\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,                                                           # 입력 텍스트\n",
    "        add_special_tokens      = True,                                 # 특수 토큰 추가\n",
    "        max_length              = max_len,                              # 최대 길이로 패딩 및 트렁케이션\n",
    "        return_token_type_ids   = False,                                # 토큰 타입 ID 반환 안 함\n",
    "        padding                 = 'max_length',                         # 최대 길이로 패딩\n",
    "        truncation              = True,                                 # 최대 길이를 초과하면 트렁케이션\n",
    "        return_attention_mask   = True,                                 # 어텐션 마스크 반환\n",
    "        return_tensors          = 'pt',                                 # PyTorch 텐서로 반환\n",
    "    )\n",
    "    # 인코딩된 입력 데이터 추출\n",
    "    input_ids      = encoding['input_ids']                              # 입력 토큰 ID\n",
    "    attention_mask = encoding['attention_mask']                         # 어텐션 마스크\n",
    "    \n",
    "    # 모델을 사용하여 예측 수행 (그래디언트 계산 비활성화)\n",
    "    with torch.no_grad():\n",
    "        outputs    = model(input_ids, attention_mask=attention_mask)    # 모델에 입력 데이터 전달\n",
    "    \n",
    "    # 모델의 출력 logits 추출\n",
    "    logits     = outputs.logits                                         # 예측된 로짓\n",
    "    prediction = torch.argmax(logits, dim=-1)                           # 가장 높은 확률의 인덱스 선택\n",
    "    \n",
    "    return prediction.item()                                            # 예측된 라벨 인덱스 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "084697d2-a7d6-4c60-9fa0-09202c33e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트할 텍스트 입력\n",
    "text_to_predict = '''삼성전자 핸드폰 고장 반품요청 물밀듯 들어와'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c124a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 토큰화\n",
    "encoding = tokenizer.encode_plus(\n",
    "    text_to_predict,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3de85c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 악재\n"
     ]
    }
   ],
   "source": [
    "# 모델을 GPU로 이동\n",
    "model = model.to('cuda')\n",
    "\n",
    "# 입력 데이터도 GPU로 이동\n",
    "input_ids = encoding['input_ids'].to('cuda')\n",
    "attention_mask = encoding['attention_mask'].to('cuda')\n",
    "\n",
    "# 모델 예측 실행\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# 예측 결과 인덱스 계산\n",
    "logits = outputs.logits\n",
    "predicted_label_index = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# 예측 결과 출력\n",
    "predicted_label = label_map[predicted_label_index]\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcab95",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b9dadc1-07ac-482d-b792-733d941561a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights have been saved as News_Samsung_KcELECTRA_model.h5\n"
     ]
    }
   ],
   "source": [
    "# 모델의 가중치 저장\n",
    "model_weights = model.state_dict()\n",
    "\n",
    "# h5 파일로 저장\n",
    "with h5py.File('../saved_models/News_Samsung_KcELECTRA.h5', 'w') as h5_file:\n",
    "    for key, value in model_weights.items():\n",
    "        h5_file.create_dataset(key, data=value.cpu().numpy())\n",
    "\n",
    "print(\"Model weights have been saved as News_Samsung_KcELECTRA.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 가중치 저장\n",
    "model_weights = model.state_dict()\n",
    "\n",
    "# pkl 파일로 저장\n",
    "torch.save(model_weights, '../saved_models/News_Samsung_KcELECTRA.pkl')\n",
    "print(\"Model weights have been saved as News_Samsung_KcELECTRA.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
